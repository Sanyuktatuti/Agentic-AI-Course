{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "## LANGSMITH TRACKING AND TRACING\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"True\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGCHAIN_PROJECT: Agentic 2.0\n",
      "LANGCHAIN_API_KEY: lsv2_\n"
     ]
    }
   ],
   "source": [
    "print(\"LANGCHAIN_PROJECT:\", os.environ.get(\"LANGCHAIN_PROJECT\"))\n",
    "print(\"LANGCHAIN_API_KEY:\", os.environ.get(\"LANGCHAIN_API_KEY\")[:5])  # Just to check it's loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x1601e0550> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x1601e02d0> root_client=<openai.OpenAI object at 0x1601e0050> root_async_client=<openai.AsyncOpenAI object at 0x1601e1090> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"What is Agentic AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result=  content='Agentic AI refers to artificial intelligence systems that possess a degree of agency, meaning they can make autonomous decisions and take actions to achieve specific goals. These systems are designed to operate independently within certain constraints and guidelines, often making decisions based on real-time data and pre-programmed objectives.\\n\\nAgentic AI is typically characterized by the following features:\\n\\n1. **Autonomy**: The ability to operate without direct human intervention, making decisions based on predefined algorithms and learning from environmental inputs.\\n\\n2. **Goal-Driven**: These AIs are programmed with specific objectives they are expected to achieve, using whichever tools or data are available to them.\\n\\n3. **Adaptability**: Agentic AIs can adjust their actions in response to changes in their environment, often through learning algorithms that allow them to improve their performance over time.\\n\\n4. **Interactivity**: The capability to interact with other systems or agents, adapting to complex multi-agent environments and making decisions that consider the actions and goals of others.\\n\\nAgentic AI systems are used in various domains, including robotics, autonomous vehicles, virtual assistants, and advanced simulation environments, where decision-making and adaptation are critical.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 231, 'prompt_tokens': 12, 'total_tokens': 243, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-BjlqP1kjqXjyTWqYI08YRFrBIABfS', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--c53adec2-8b13-4be4-9aeb-385fac8d748d-0' usage_metadata={'input_tokens': 12, 'output_tokens': 231, 'total_tokens': 243, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "result content=  Agentic AI refers to artificial intelligence systems that possess a degree of agency, meaning they can make autonomous decisions and take actions to achieve specific goals. These systems are designed to operate independently within certain constraints and guidelines, often making decisions based on real-time data and pre-programmed objectives.\n",
      "\n",
      "Agentic AI is typically characterized by the following features:\n",
      "\n",
      "1. **Autonomy**: The ability to operate without direct human intervention, making decisions based on predefined algorithms and learning from environmental inputs.\n",
      "\n",
      "2. **Goal-Driven**: These AIs are programmed with specific objectives they are expected to achieve, using whichever tools or data are available to them.\n",
      "\n",
      "3. **Adaptability**: Agentic AIs can adjust their actions in response to changes in their environment, often through learning algorithms that allow them to improve their performance over time.\n",
      "\n",
      "4. **Interactivity**: The capability to interact with other systems or agents, adapting to complex multi-agent environments and making decisions that consider the actions and goals of others.\n",
      "\n",
      "Agentic AI systems are used in various domains, including robotics, autonomous vehicles, virtual assistants, and advanced simulation environments, where decision-making and adaptation are critical.\n"
     ]
    }
   ],
   "source": [
    "print(\"result= \", result)\n",
    "print(\"result content= \",result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"\\n<think>\\nOkay, the user introduced themselves as Sanyukta. I should respond politely and maybe ask a question to keep the conversation going. Let me make sure my tone is friendly and open. Maybe I can ask how they're doing today or if there's anything they'd like to talk about. I need to avoid any markdown and keep it natural. Let me check the previous messages to ensure continuity. Alright, I'll go with a greeting and a question to engage them.\\n</think>\\n\\nHello, Sanyukta! Nice to meet you. ðŸ˜Š How are you today? Is there anything interesting you'd like to share or talk about?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 130, 'prompt_tokens': 19, 'total_tokens': 149, 'completion_time': 0.301818755, 'prompt_time': 0.004042868, 'queue_time': 0.6630890930000001, 'total_time': 0.305861623}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_6b36369676', 'finish_reason': 'stop', 'logprobs': None} id='run--bba73afe-34cb-4f58-9c73-9b4a7e507802-0' usage_metadata={'input_tokens': 19, 'output_tokens': 130, 'total_tokens': 149}\n",
      "content='\\n<think>\\nOkay, the user just introduced herself as Sanyukta. I should respond politely. Let me start by greeting her back. Maybe say \"Hello, Sanyukta!\" to acknowledge her name. Then, I can offer assistance. I should keep it friendly and open-ended so she feels comfortable to ask for help. Maybe something like, \"How can I assist you today?\" That way, she knows I\\'m here to help with whatever she needs. Let me check if that\\'s clear and friendly enough. Yeah, that should work. I\\'ll go with that.\\n</think>\\n\\nHello, Sanyukta! How can I assist you today? ðŸ˜Š' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 134, 'prompt_tokens': 19, 'total_tokens': 153, 'completion_time': 0.309819753, 'prompt_time': 0.004259075, 'queue_time': 0.065712445, 'total_time': 0.314078828}, 'model_name': 'qwen-qwq-32b', 'system_fingerprint': 'fp_6b36369676', 'finish_reason': 'stop', 'logprobs': None} id='run--a99bc121-6154-467e-871e-b3c27983f526-0' usage_metadata={'input_tokens': 19, 'output_tokens': 134, 'total_tokens': 153}\n",
      "\n",
      "<think>\n",
      "Okay, the user just introduced herself as Sanyukta. I should respond politely. Let me start by greeting her back. Maybe say \"Hello, Sanyukta!\" to acknowledge her name. Then, I can offer assistance. I should keep it friendly and open-ended so she feels comfortable to ask for help. Maybe something like, \"How can I assist you today?\" That way, she knows I'm here to help with whatever she needs. Let me check if that's clear and friendly enough. Yeah, that should work. I'll go with that.\n",
      "</think>\n",
      "\n",
      "Hello, Sanyukta! How can I assist you today? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"qwen-qwq-32b\")\n",
    "print(model.invoke(\"Hi, my name is Sanyukta\"))\n",
    "result = model.invoke(\"Hi, my name is Sanyukta\")\n",
    "\n",
    "print(result)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering - Insturction to an LLM model how it should behave\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI engineer. Provide me answer based on the question\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt\n",
    "\n",
    "# what is the difference between system and user messages?\n",
    "# system message is the instruction to the model about how it should behave\n",
    "# user message is the input to the model\n",
    "\n",
    "# how to use the prompt template?\n",
    "# prompt.format_messages(input=\"What is the difference between system and user messages?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model = ChatGroq(model=\"gemma2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI engineer. Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x160a10fc0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x160a11940>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### chaining = combines prompt to the LLM model\n",
    "chain = prompt | model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You're asking about **Langsmith**, an exciting tool in the AI world! \\n\\nHere's what I can tell you:\\n\\n**Langsmith is an open-source platform designed to simplify the process of building and deploying AI applications, particularly those powered by large language models (LLMs).**\\n\\nThink of it as a toolbox filled with pre-built components and functionalities specifically tailored for working with LLMs. \\n\\n**Here are some key features of Langsmith:**\\n\\n* **Modular Design:** It breaks down the LLM development process into manageable modules, making it easier to understand and customize.\\n* **Streamlined Workflow:** Langsmith provides a user-friendly interface and automation features to accelerate the development cycle.\\n* **Fine-Tuning Capabilities:** It allows you to fine-tune existing LLMs on your specific datasets, leading to more accurate and relevant results for your applications.\\n* **Model Management:** Langsmith simplifies the process of managing and deploying different LLM models.\\n* **Community Driven:** Being open-source, Langsmith benefits from a vibrant community of developers who contribute to its growth and improvement.\\n\\n**Why is Langsmith significant?**\\n\\nIt democratizes access to LLM technology by making it more accessible to developers and researchers who may not have the resources or expertise to build complex AI systems from scratch. This opens up possibilities for a wider range of applications across various industries.\\n\\n**Want to learn more?**\\n\\nI recommend checking out the official Langsmith website and documentation for in-depth information and tutorials. You'll find a wealth of resources to get started with this powerful platform.\\n\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"input\": \"Can you tell me something about Langsmith?\"})\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's talk about Langsmith!  \\n\\nLangsmith is a powerful open-source framework built by the amazing folks at Google DeepMind.  Think of it as a toolbox specifically designed to make building and working with large language models (LLMs) easier and more efficient.  \\n\\nHere's a breakdown of what makes Langsmith special:\\n\\n**Key Features:**\\n\\n* **Modular and Extensible:** Langsmith is designed with modularity in mind.  This means you can easily add or swap out components to customize your LLM workflow. Need to integrate a specific dataset or fine-tuning technique? Langsmith lets you do it.\\n\\n* **Streamlined Development:**  It provides a structured way to organize your LLM projects, making it simpler to manage code, datasets, and training configurations.\\n\\n* **Hardware Agnostic:**  Langsmith can work with a variety of hardware setups, from your local machine to powerful cloud GPUs.\\n\\n* **Open and Collaborative:**  Being open-source means the community can contribute to its development, share tools, and learn from each other. This fosters rapid innovation and improvement.\\n\\n**What Can You Do with Langsmith?**\\n\\n* **Fine-tuning:**  Adjust pre-trained LLMs to perform specific tasks, like question answering, summarization, or code generation.\\n* **Experimentation:**  Easily test different model architectures, training techniques, and datasets to see what works best for your use case.\\n* **Deployment:**  Langsmith can help you deploy your fine-tuned LLMs for real-world applications.\\n\\n**Getting Started:**\\n\\nIf you're interested in exploring Langsmith, I highly recommend checking out the official documentation and community resources:\\n\\n* **Website:** [https://github.com/google-deepmind/langsmith](https://github.com/google-deepmind/langsmith)\\n* **Documentation:** [https://langsmith.readthedocs.io/en/latest/](https://langsmith.readthedocs.io/en/latest/)\\n\\n\\nLet me know if you have any other questions about Langsmith or LLMs in general.  I'm here to help!\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Output Parser: Way to display the output in a specific format\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "response = chain.invoke({\"input\": \"Can you tell me something about Langsmith?\"})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = \"Answer the user query. \\n {format_instruction} \\n {query} \\n\",\n",
    "    input_variables = [\"query\"],\n",
    "    partial_variables = {\"format_instruction\": output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'Return a JSON object.'}, template='Answer the user query. \\n {format_instruction} \\n {query} \\n')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Langsmith is an open-source platform for building and deploying AI applications, particularly focusing on large language models (LLMs).', 'key_features': ['Modular and extensible design', 'Supports various LLMs and other AI models', 'Provides tools for fine-tuning and evaluating models', 'Offers a user-friendly interface for building and managing AI workflows', 'Facilitates collaboration and sharing of AI models and applications'], 'developer_community': 'Langsmith has a growing community of developers and researchers contributing to its development and expanding its capabilities.'}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | model | output_parser\n",
    "response = chain.invoke({\"query\": \"Can you tell me something about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between ChatPromptTemplate and PromptTemplate\n",
    "1. PromptTemplate\n",
    "- Purpose:\n",
    "Used for formatting prompts for standard (text) LLMs.\n",
    "- Input/Output:\n",
    "Takes a string template and fills in variables to produce a single string prompt.\n",
    "- Use case:\n",
    "For models that expect a single string as input (e.g., GPT-3, GPT-4 in completion mode).\n",
    "Example:\n",
    "Apply to Getting-Star...\n",
    "\"\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "2. ChatPromptTemplate\n",
    "- Purpose:\n",
    "Used for formatting prompts for chat-based LLMs (like OpenAI ChatGPT, GPT-4o, etc.).\n",
    "- Input/Output:\n",
    "Takes a list of message templates (system, user, assistant, etc.) and fills in variables to produce a list of message objects.\n",
    "- Use case:\n",
    "For models that expect a sequence of chat messages as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': \"The concept of 'law of the land' is a complex one, often used to refer to the supreme law of a particular jurisdiction.  \\n\\n  In the United States, the **Constitution** is considered the highest law of the land. It establishes the framework for the government and outlines the fundamental rights of citizens. Federal laws, passed by Congress, and state laws, passed by state legislatures, must comply with the Constitution. \\n\\n  The phrase 'rule of law' is often used interchangeably with 'law of the land'. It signifies that everyone, including government officials, is subject to the law and that laws are applied fairly and consistently.\\n\\n  It's important to note that the specific laws governing a particular situation can vary depending on the jurisdiction and the specific circumstances.\"}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using ChatPromptTemplate\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a Law Expert. Answer the user query based on the law.{format_instructions}\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "response = chain.invoke({\"input\": \"What is the law of the land?\",\n",
    "                         \"format_instructions\": output_parser.get_format_instructions()})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Langsmith': [{'description': 'Langsmith is an open-source platform for developing and deploying AI assistants.'},\n",
       "  {'features': [{'feature': 'Modular design allows for customization and extensibility.'},\n",
       "    {'feature': 'Supports multiple programming languages.'},\n",
       "    {'feature': 'Provides tools for training, evaluating, and deploying models.'}]},\n",
       "  {'use_cases': [{'use_case': 'Building chatbots and conversational agents.'},\n",
       "    {'use_case': 'Automating tasks and workflows.'},\n",
       "    {'use_case': 'Personalizing user experiences.'}]}]}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert AI engineer. The output should be in XML format. {format_instruction}\"),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "response = chain.invoke({\"input\": \"Can you tell me something about Langsmith?\",\n",
    "                         \"format_instruction\": output_parser.get_format_instructions()\n",
    "                         })\n",
    "\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why couldn't the bicycle find its way home?\",\n",
       " 'punchline': 'Because it lost its bearings!'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with Pydantic\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model = ChatOpenAI(temperature=0.7)\n",
    "\n",
    "#Define you rdesired Data Structure\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"Question to set up a Joke\")\n",
    "    punchline: str = Field(description=\"Answer to receive the joke\")\n",
    "\n",
    "# And a query intended to frompt a language model to populate the data structure\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "#set up a parser + inject instructions into prompt template\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query. \\n {format_instruction} \\n {query} \\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "response = chain.invoke({\"query\": joke_query})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response': \"Sure! Here's a joke for you: Why couldn't the bicycle stand up by itself? Because it was two tired!\"}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# without Pydantic\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = \"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables = [\"query\"],\n",
    "    partial_variables = {\"format_instructions\": output_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<movies>\n",
      "1. Forrest Gump (1994)\n",
      "2. Saving Private Ryan (1998)\n",
      "3. Cast Away (2000)\n",
      "4. The Green Mile (1999)\n",
      "5. Big (1988)\n",
      "6. Philadelphia (1993)\n",
      "7. Sleepless in Seattle (1993)\n",
      "8. Apollo 13 (1995)\n",
      "9. Captain Phillips (2013)\n",
      "10. Toy Story (1995)\n",
      "</movies>\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "\n",
    "actor_query = \"generate the shortened fimlmography for Tom Hanks\"\n",
    "\n",
    "output = model.invoke(\n",
    "    f\"\"\"{actor_query}\n",
    "Please enclose the movies in <movies></movies> tags\"\"\"\n",
    ")\n",
    "\n",
    "print (output.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Parsers Langchain Documentation\n",
    "\n",
    "https://python.langchain.com/docs/concepts/output_parsers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASSIGNMENT\n",
    "\n",
    "Create a simple assistant that uses LLM in Pydantic that takes a query of a product should give 2 info -> {Product  name: \" \"}, {Product Details: \" \"} and {Tentative Price: USD \"int or \"float\"}. Use ChatPromptTemplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Query: I want to buy iPhone 16 pro max?\n",
      "==================================================\n",
      "Result: iPhone 16 Pro Max\n",
      "Product Details: The iPhone 16 Pro Max is the latest flagship smartphone from Apple, featuring a large display, powerful camera system, and top-of-the-line performance.\n",
      "Tentative Price: None\n",
      "\n",
      "==================================================\n",
      "Query: I want to buy MacBook Pro M4 Max?\n",
      "==================================================\n",
      "Result: MacBook Pro M4 Max\n",
      "Product Details: The MacBook Pro M4 Max is a high-performance laptop with a stunning Retina display, powerful M1 chip, and long battery life. It is perfect for professionals and creatives who require top-of-the-line performance.\n",
      "Tentative Price: None\n",
      "\n",
      "==================================================\n",
      "Query: I am looking for aSamsung Galaxy S23?\n",
      "==================================================\n",
      "Error: Failed to parse Product from completion {\"properties\": {\"product_name\": {\"title\": \"Product Name\", \"description\": \"The name of the Product. \", \"type\": \"string\"}, \"product_details\": {\"title\": \"Product Details\", \"description\": \"The details of the Product. \", \"type\": \"string\"}, \"tentative_price\": {\"title\": \"Tentative Price\", \"description\": \"The tentative price of the product in USD (provide a realistic price). \", \"anyOf\": [{\"type\": \"integer\"}, {\"type\": \"number\"}, {\"type\": \"null\"}]}}, \"required\": [\"product_name\", \"product_details\", \"tentative_price\"]}. Got: 3 validation errors for Product\n",
      "product_name\n",
      "  Field required [type=missing, input_value={'properties': {'product_...ls', 'tentative_price']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "product_details\n",
      "  Field required [type=missing, input_value={'properties': {'product_...ls', 'tentative_price']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "tentative_price\n",
      "  Field required [type=missing, input_value={'properties': {'product_...ls', 'tentative_price']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/missing\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Union, Optional\n",
    "\n",
    "model = ChatOpenAI(temperature = 0.7)\n",
    "\n",
    "class Product(BaseModel):\n",
    "    product_name: str = Field(description = \"The name of the Product. \")\n",
    "    product_details: str = Field(description = \"The details of the Product. \")\n",
    "    tentative_price: Optional[Union[int,float]] = Field(description = \"The tentative price of the product in USD (provide a realistic price). \")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object = Product)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant that can answer questions about the product and provide answer in a specified format. {format_instructions}\"),\n",
    "        (\"user\", \"{query}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "# Running the assistant\n",
    "\n",
    "test_queries = [\"I want to buy iPhone 16 pro max?\", \n",
    "                \"I want to buy MacBook Pro M4 Max?\", \n",
    "                \"I am looking for aSamsung Galaxy S23?\"\n",
    "            ]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    try:\n",
    "        result  = chain.invoke({\n",
    "            \"query\": query,\n",
    "            \"format_instructions\": parser.get_format_instructions()\n",
    "        })\n",
    "\n",
    "        print(f\"Result: {result.product_name}\")\n",
    "        print(f\"Product Details: {result.product_details}\")\n",
    "        print(f\"Tentative Price: {result.tentative_price}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Test with your own queries\n",
    "\n",
    "def get_product_info(query: str):\n",
    "    \"\"\"\n",
    "    Function to get product information for any query.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = chain.invoke({\n",
    "            \"query\": query,\n",
    "            \"format_instructions\": parser.get_format_instructions()\n",
    "        })\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    \n",
    "#Example Usage:\n",
    "# result = get_product_info(\"tell me about Macbook Air M2\")\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Welcome to the product assistant\n",
      "============================================================\n",
      "Ask mem about any product and I shall provide:\n",
      "1. Product Name\n",
      "2. Product Details\n",
      "3. Tentative Price (if available)\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "Query: Phillips Toaster\n",
      "==================================================\n",
      "Result: Phillips Toaster\n",
      "Product Details: This Phillips Toaster features a stylish design with wide slots for various types of bread. It has multiple browning settings and a cancel button for convenience. The toaster also has a removable crumb tray for easy cleaning.\n",
      "Tentative Price: None USD\n",
      "\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Query: HP Pavillion\n",
      "==================================================\n",
      "Result: HP Pavillion\n",
      "Product Details: \n",
      "Tentative Price: None USD\n",
      "\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Query: Rare Beauty Lip Tint\n",
      "==================================================\n",
      "Result: Rare Beauty Lip Tint\n",
      "Product Details: A lightweight lip tint that provides a natural flush of color to the lips. It is long-lasting and comfortable to wear.\n",
      "Tentative Price: None USD\n",
      "\n",
      "==================================================\n",
      "\n",
      " Thank you for using the product assistant. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Interactive version. Do run the above code first.\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*60)\n",
    "    print(\"Welcome to the product assistant\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(\"Ask mem about any product and I shall provide:\")\n",
    "    print(\"1. Product Name\")\n",
    "    print(\"2. Product Details\")\n",
    "    print(\"3. Tentative Price (if available)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    while True:\n",
    "        # User Input\n",
    "        user_query =input(\"\\n Enter your product query: \")\n",
    "\n",
    "        #checking if the query is empty\n",
    "\n",
    "        if user_query.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"\\n Thank you for using the product assistant. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        #Check if the query is empty\n",
    "        if not user_query:\n",
    "            print(\"Please enter a valid query.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Query: {user_query}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        # Display the result\n",
    "\n",
    "        try:\n",
    "            result = get_product_info(user_query)\n",
    "\n",
    "            if isinstance(result, str) and result.startswith(\"Error\"):\n",
    "                print(f\"\\n{result}\")\n",
    "            else:\n",
    "                print(f\"Result: {result.product_name}\")\n",
    "                print(f\"Product Details: {result.product_details}\")\n",
    "                print(f\"Tentative Price: {result.tentative_price} USD\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "        print(f\"\\n{'='*50}\")\n",
    "\n",
    "# Run the interactive program\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
